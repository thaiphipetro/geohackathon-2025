{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GeoHackathon 2025: Training Data Exploration\n",
    "\n",
    "**Purpose:** Explore the well completion reports to understand:\n",
    "- Document structure and content\n",
    "- Where MD, TVD, ID parameters are located\n",
    "- Table formats and variations\n",
    "- OCR requirements\n",
    "- Vision model opportunities\n",
    "\n",
    "**Dataset:** 8 wells from Dutch geothermal projects (nlog.nl)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install required packages (run once)\n",
    "!pip install docling docling[rapidocr] pandas openpyxl tabulate rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import Dict, List\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Pretty printing\n",
    "from rich import print as rprint\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "from rich.panel import Panel\n",
    "from rich.markdown import Markdown\n",
    "\n",
    "console = Console()\n",
    "\n",
    "print(\"âœ“ Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Set paths\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "DATA_DIR = PROJECT_ROOT / \"Training data-shared with participants\"\n",
    "OUTPUT_DIR = PROJECT_ROOT / \"outputs\" / \"exploration\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Data exists: {DATA_DIR.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Scan dataset structure\n",
    "def scan_dataset(data_dir: Path) -> Dict:\n",
    "    \"\"\"Scan training data directory\"\"\"\n",
    "    \n",
    "    wells = {}\n",
    "    \n",
    "    # Find all well folders\n",
    "    well_folders = sorted([d for d in data_dir.iterdir() if d.is_dir() and d.name.startswith('Well')])\n",
    "    \n",
    "    for well_folder in well_folders:\n",
    "        well_name = well_folder.name\n",
    "        \n",
    "        # Count files by type\n",
    "        pdf_files = list(well_folder.rglob(\"*.pdf\"))\n",
    "        xlsx_files = list(well_folder.rglob(\"*.xlsx\"))\n",
    "        docx_files = list(well_folder.rglob(\"*.docx\"))\n",
    "        png_files = list(well_folder.rglob(\"*.png\"))\n",
    "        \n",
    "        # Find well reports specifically\n",
    "        well_reports = [f for f in pdf_files if 'well report' in str(f.parent).lower()]\n",
    "        eowr_files = [f for f in well_reports if 'eowr' in f.name.lower()]\n",
    "        \n",
    "        wells[well_name] = {\n",
    "            \"path\": well_folder,\n",
    "            \"pdf_count\": len(pdf_files),\n",
    "            \"xlsx_count\": len(xlsx_files),\n",
    "            \"docx_count\": len(docx_files),\n",
    "            \"png_count\": len(png_files),\n",
    "            \"well_reports\": well_reports,\n",
    "            \"eowr_files\": eowr_files,\n",
    "            \"subfolders\": [d.name for d in well_folder.iterdir() if d.is_dir()]\n",
    "        }\n",
    "    \n",
    "    return wells\n",
    "\n",
    "wells_data = scan_dataset(DATA_DIR)\n",
    "print(f\"Found {len(wells_data)} wells\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Display dataset summary table\n",
    "summary_table = Table(title=\"Training Data Summary\", show_lines=True)\n",
    "summary_table.add_column(\"Well\", style=\"cyan\", no_wrap=True)\n",
    "summary_table.add_column(\"PDFs\", justify=\"right\")\n",
    "summary_table.add_column(\"Excel\", justify=\"right\")\n",
    "summary_table.add_column(\"Word\", justify=\"right\")\n",
    "summary_table.add_column(\"Images\", justify=\"right\")\n",
    "summary_table.add_column(\"EOWR Reports\", justify=\"right\")\n",
    "summary_table.add_column(\"Subfolders\", style=\"dim\")\n",
    "\n",
    "for well_name, data in wells_data.items():\n",
    "    summary_table.add_row(\n",
    "        well_name,\n",
    "        str(data['pdf_count']),\n",
    "        str(data['xlsx_count']),\n",
    "        str(data['docx_count']),\n",
    "        str(data['png_count']),\n",
    "        str(len(data['eowr_files'])),\n",
    "        \", \".join(data['subfolders'])\n",
    "    )\n",
    "\n",
    "console.print(summary_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Show total statistics\n",
    "total_pdfs = sum(d['pdf_count'] for d in wells_data.values())\n",
    "total_excel = sum(d['xlsx_count'] for d in wells_data.values())\n",
    "total_word = sum(d['docx_count'] for d in wells_data.values())\n",
    "total_images = sum(d['png_count'] for d in wells_data.values())\n",
    "total_eowr = sum(len(d['eowr_files']) for d in wells_data.values())\n",
    "\n",
    "console.print(Panel(\n",
    "    f\"\"\"[bold green]Dataset Statistics:[/bold green]\n",
    "    \n",
    "Total Wells: {len(wells_data)}\n",
    "Total PDF Files: {total_pdfs}\n",
    "Total Excel Files: {total_excel}\n",
    "Total Word Files: {total_word}\n",
    "Total Images: {total_images}\n",
    "Total EOWR Reports: {total_eowr}\n",
    "\"\"\",\n",
    "    title=\"ðŸ“Š Overall Summary\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Explore Master Index (boreholes.xlsx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load boreholes master file\n",
    "boreholes_file = DATA_DIR / \"boreholes.xlsx\"\n",
    "\n",
    "if boreholes_file.exists():\n",
    "    df_boreholes = pd.read_excel(boreholes_file)\n",
    "    \n",
    "    console.print(\"[bold green]Boreholes Master Index:[/bold green]\")\n",
    "    print(f\"Shape: {df_boreholes.shape}\")\n",
    "    print(f\"Columns: {list(df_boreholes.columns)}\")\n",
    "    print()\n",
    "    display(df_boreholes.head(10))\n",
    "else:\n",
    "    console.print(\"[red]boreholes.xlsx not found[/red]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Parse Well Report with Docling\n",
    "\n",
    "Let's start with **Well 5 (NLW-GT-03)** - the best documented well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize Docling parser\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions, OcrOptions\n",
    "\n",
    "# Configure with OCR support\n",
    "pipeline_options = PdfPipelineOptions()\n",
    "pipeline_options.do_ocr = True\n",
    "pipeline_options.ocr_options = OcrOptions(\n",
    "    engine=\"rapidocr\",  # CPU-friendly OCR\n",
    "    force_ocr=False,    # Auto-detect if OCR needed\n",
    ")\n",
    "\n",
    "converter = DocumentConverter(\n",
    "    format_options={\n",
    "        \"pdf\": PdfFormatOption(pipeline_options=pipeline_options)\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"âœ“ Docling parser initialized with OCR support\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Select Well 5 EOWR report\n",
    "well_5_data = wells_data['Well 5']\n",
    "\n",
    "console.print(f\"[bold cyan]Well 5 EOWR Reports:[/bold cyan]\")\n",
    "for i, eowr in enumerate(well_5_data['eowr_files']):\n",
    "    print(f\"{i+1}. {eowr.name}\")\n",
    "    print(f\"   Size: {eowr.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Parse the first EOWR report\n",
    "if well_5_data['eowr_files']:\n",
    "    test_pdf = well_5_data['eowr_files'][0]\n",
    "    \n",
    "    console.print(f\"[bold green]Parsing:[/bold green] {test_pdf.name}\")\n",
    "    console.print(\"[yellow]This may take 1-2 minutes...[/yellow]\")\n",
    "    \n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Parse\n",
    "    result = converter.convert(str(test_pdf))\n",
    "    doc = result.document\n",
    "    \n",
    "    parse_time = time.time() - start_time\n",
    "    \n",
    "    console.print(f\"[green]âœ“ Parsing complete in {parse_time:.1f}s[/green]\")\n",
    "else:\n",
    "    console.print(\"[red]No EOWR files found for Well 5[/red]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Show parsing statistics\n",
    "if 'doc' in locals():\n",
    "    text = doc.export_to_markdown()\n",
    "    \n",
    "    stats_table = Table(title=\"Parsing Results\")\n",
    "    stats_table.add_column(\"Metric\", style=\"cyan\")\n",
    "    stats_table.add_column(\"Value\", style=\"magenta\")\n",
    "    \n",
    "    stats_table.add_row(\"Pages\", str(len(doc.pages)) if hasattr(doc, 'pages') else 'N/A')\n",
    "    stats_table.add_row(\"Text Length\", f\"{len(text):,} characters\")\n",
    "    stats_table.add_row(\"Tables Found\", str(len(doc.tables)) if hasattr(doc, 'tables') else '0')\n",
    "    stats_table.add_row(\"Images Found\", str(len(doc.pictures)) if hasattr(doc, 'pictures') else '0')\n",
    "    stats_table.add_row(\"OCR Used\", str(getattr(doc, 'has_ocr_content', False)))\n",
    "    stats_table.add_row(\"Parse Time\", f\"{parse_time:.1f}s\")\n",
    "    \n",
    "    console.print(stats_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Explore Extracted Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Show first 2000 characters\n",
    "if 'text' in locals():\n",
    "    console.print(Panel(\n",
    "        Markdown(text[:2000] + \"\\n\\n...[truncated]...\"),\n",
    "        title=\"ðŸ“„ Document Text Preview (first 2000 chars)\",\n",
    "        expand=False\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Search for well identifiers\n",
    "import re\n",
    "\n",
    "if 'text' in locals():\n",
    "    # Look for well names\n",
    "    well_patterns = [\n",
    "        r'NLW-GT-\\d+(?:-S\\d+)?',  # Naaldwijk wells\n",
    "        r'ADK-GT-\\d+',             # Andijk wells\n",
    "        r'HAG-GT-\\d+',             # Den Haag wells\n",
    "        r'MDM-GT-\\d+',             # Monster-Duivenvoorde wells\n",
    "        r'LIR-GT-\\d+',             # De Lier wells\n",
    "        r'BRI-GT-\\d+',             # Vierpolders wells\n",
    "    ]\n",
    "    \n",
    "    found_wells = set()\n",
    "    for pattern in well_patterns:\n",
    "        matches = re.findall(pattern, text)\n",
    "        found_wells.update(matches)\n",
    "    \n",
    "    console.print(f\"[bold green]Well identifiers found:[/bold green] {', '.join(sorted(found_wells))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Search for depth mentions\n",
    "if 'text' in locals():\n",
    "    # Look for depth values\n",
    "    depth_pattern = r'(\\d+(?:\\.\\d+)?)\\s*(?:m|meter|metre)'\n",
    "    depth_matches = re.findall(depth_pattern, text[:5000])  # First 5000 chars\n",
    "    \n",
    "    if depth_matches:\n",
    "        depths = [float(d) for d in depth_matches[:10]]  # First 10\n",
    "        console.print(f\"[bold green]Sample depth values found:[/bold green] {depths}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Explore Tables - THIS IS KEY!\n",
    "\n",
    "Tables contain the MD, TVD, ID data we need for Sub-Challenge 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Show all tables found\n",
    "if hasattr(doc, 'tables') and doc.tables:\n",
    "    console.print(f\"[bold green]Found {len(doc.tables)} tables in document[/bold green]\\n\")\n",
    "    \n",
    "    for i, table in enumerate(doc.tables[:5]):  # Show first 5 tables\n",
    "        console.print(f\"[bold cyan]â•â•â• Table {i+1} â•â•â•[/bold cyan]\")\n",
    "        \n",
    "        # Show page number if available\n",
    "        if hasattr(table, 'page'):\n",
    "            console.print(f\"Page: {table.page}\")\n",
    "        \n",
    "        # Try to extract table data\n",
    "        if hasattr(table, 'data') and table.data:\n",
    "            # Convert to DataFrame for display\n",
    "            try:\n",
    "                df_table = pd.DataFrame(table.data)\n",
    "                \n",
    "                # Use first row as headers if it looks like headers\n",
    "                if len(df_table) > 0:\n",
    "                    if hasattr(table, 'headers') and table.headers:\n",
    "                        df_table.columns = table.headers\n",
    "                    \n",
    "                    console.print(f\"Shape: {df_table.shape}\")\n",
    "                    display(df_table.head(10))\n",
    "                    print()\n",
    "            except Exception as e:\n",
    "                console.print(f\"[red]Could not display table: {e}[/red]\")\n",
    "        \n",
    "        print(\"\\n\")\n",
    "else:\n",
    "    console.print(\"[yellow]No tables found in document[/yellow]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Search for casing/completion tables specifically\n",
    "if hasattr(doc, 'tables') and doc.tables:\n",
    "    console.print(\"[bold cyan]ðŸ” Searching for Casing/Completion Tables...[/bold cyan]\\n\")\n",
    "    \n",
    "    # Keywords that indicate casing tables\n",
    "    casing_keywords = ['casing', 'completion', 'md', 'tvd', 'depth', 'diameter', 'id', 'od', 'tubular']\n",
    "    \n",
    "    for i, table in enumerate(doc.tables):\n",
    "        # Check if table text contains casing keywords\n",
    "        if hasattr(table, 'data') and table.data:\n",
    "            # Convert table to string for searching\n",
    "            table_str = str(table.data).lower()\n",
    "            \n",
    "            # Check if contains relevant keywords\n",
    "            keyword_matches = [kw for kw in casing_keywords if kw in table_str]\n",
    "            \n",
    "            if len(keyword_matches) >= 2:  # At least 2 keywords\n",
    "                console.print(f\"[green]âœ“ Potential casing table found! (Table {i+1})[/green]\")\n",
    "                console.print(f\"  Keywords matched: {', '.join(keyword_matches)}\")\n",
    "                \n",
    "                # Display this table\n",
    "                try:\n",
    "                    df_table = pd.DataFrame(table.data)\n",
    "                    if hasattr(table, 'headers') and table.headers:\n",
    "                        df_table.columns = table.headers\n",
    "                    display(df_table.head(10))\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Parsed Output for Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save full text to file\n",
    "if 'text' in locals():\n",
    "    output_file = OUTPUT_DIR / f\"{test_pdf.stem}_text.md\"\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(text)\n",
    "    \n",
    "    console.print(f\"[green]âœ“ Full text saved to:[/green] {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save tables to CSV for easy inspection\n",
    "if hasattr(doc, 'tables') and doc.tables:\n",
    "    for i, table in enumerate(doc.tables):\n",
    "        if hasattr(table, 'data') and table.data:\n",
    "            try:\n",
    "                df_table = pd.DataFrame(table.data)\n",
    "                if hasattr(table, 'headers') and table.headers:\n",
    "                    df_table.columns = table.headers\n",
    "                \n",
    "                # Save to CSV\n",
    "                table_file = OUTPUT_DIR / f\"{test_pdf.stem}_table_{i+1}.csv\"\n",
    "                df_table.to_csv(table_file, index=False)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    console.print(f\"[green]âœ“ Tables saved to:[/green] {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Explore NodalAnalysis.py Script\n",
    "\n",
    "Let's see exactly what format the script expects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Read the nodal analysis script\n",
    "nodal_script = DATA_DIR / \"NodalAnalysis.py\"\n",
    "\n",
    "if nodal_script.exists():\n",
    "    with open(nodal_script, 'r') as f:\n",
    "        script_content = f.read()\n",
    "    \n",
    "    # Show the well trajectory section\n",
    "    console.print(Panel(\n",
    "        Markdown(\"```python\\n\" + script_content[400:800] + \"\\n```\"),\n",
    "        title=\"ðŸ“œ NodalAnalysis.py - Well Trajectory Format\"\n",
    "    ))\n",
    "else:\n",
    "    console.print(\"[red]NodalAnalysis.py not found[/red]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Extract the exact format needed\n",
    "console.print(\"[bold cyan]Required Input Format for NodalAnalysis.py:[/bold cyan]\\n\")\n",
    "console.print(\"\"\"well_trajectory = [\n",
    "    {\"MD\": 0.0,    \"TVD\": 0.0,    \"ID\": 0.3397},  # in meters\n",
    "    {\"MD\": 500.0,  \"TVD\": 500.0,  \"ID\": 0.2445},\n",
    "    {\"MD\": 1500.0, \"TVD\": 1500.0, \"ID\": 0.1778},\n",
    "    {\"MD\": 2500.0, \"TVD\": 2500.0, \"ID\": 0.1778},\n",
    "]\"\"\")\n",
    "\n",
    "console.print(\"\\n[bold yellow]Our Task:[/bold yellow]\")\n",
    "console.print(\"Extract MD, TVD, ID arrays from well reports and format exactly like this!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Quick Comparison: Multiple Wells\n",
    "\n",
    "Let's parse a few more wells to see variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Parse EOWR from Wells 1, 5, and 7 (high quality wells)\n",
    "wells_to_compare = ['Well 1', 'Well 5', 'Well 7']\n",
    "\n",
    "comparison_data = {}\n",
    "\n",
    "for well_name in wells_to_compare:\n",
    "    if well_name in wells_data and wells_data[well_name]['eowr_files']:\n",
    "        eowr_file = wells_data[well_name]['eowr_files'][0]\n",
    "        \n",
    "        console.print(f\"[cyan]Parsing {well_name}...[/cyan]\")\n",
    "        \n",
    "        try:\n",
    "            result = converter.convert(str(eowr_file))\n",
    "            doc = result.document\n",
    "            text = doc.export_to_markdown()\n",
    "            \n",
    "            comparison_data[well_name] = {\n",
    "                'pages': len(doc.pages) if hasattr(doc, 'pages') else 0,\n",
    "                'text_length': len(text),\n",
    "                'tables': len(doc.tables) if hasattr(doc, 'tables') else 0,\n",
    "                'images': len(doc.pictures) if hasattr(doc, 'pictures') else 0,\n",
    "            }\n",
    "            \n",
    "            console.print(f\"  âœ“ {comparison_data[well_name]['tables']} tables found\")\n",
    "        except Exception as e:\n",
    "            console.print(f\"  [red]âœ— Error: {e}[/red]\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Display comparison\n",
    "if comparison_data:\n",
    "    comp_table = Table(title=\"Well Document Comparison\")\n",
    "    comp_table.add_column(\"Well\")\n",
    "    comp_table.add_column(\"Pages\", justify=\"right\")\n",
    "    comp_table.add_column(\"Text Length\", justify=\"right\")\n",
    "    comp_table.add_column(\"Tables\", justify=\"right\")\n",
    "    comp_table.add_column(\"Images\", justify=\"right\")\n",
    "    \n",
    "    for well_name, data in comparison_data.items():\n",
    "        comp_table.add_row(\n",
    "            well_name,\n",
    "            str(data['pages']),\n",
    "            f\"{data['text_length']:,}\",\n",
    "            str(data['tables']),\n",
    "            str(data['images'])\n",
    "        )\n",
    "    \n",
    "    console.print(comp_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Key Findings Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate findings report\n",
    "findings = f\"\"\"\n",
    "# Data Exploration Findings\n",
    "\n",
    "## Dataset Overview\n",
    "- **Total Wells:** {len(wells_data)}\n",
    "- **Total PDFs:** {total_pdfs}\n",
    "- **EOWR Reports:** {total_eowr}\n",
    "\n",
    "## Document Parsing\n",
    "- âœ… Docling successfully parses PDFs\n",
    "- âœ… OCR works for scanned documents\n",
    "- âœ… Tables are extracted (though may need refinement)\n",
    "- âœ… Text extraction is complete\n",
    "\n",
    "## Critical Observations\n",
    "1. **Casing tables are present** in EOWR reports\n",
    "2. **Table formats vary** between wells\n",
    "3. **Column names differ** (MD vs Measured Depth, etc.)\n",
    "4. **Units may vary** (meters vs feet, inches vs mm)\n",
    "\n",
    "## Next Steps\n",
    "1. âœ… Build table extraction logic\n",
    "2. âœ… Identify MD, TVD, ID columns\n",
    "3. âœ… Handle unit conversions\n",
    "4. âœ… Format for NodalAnalysis.py\n",
    "5. âœ… Build RAG system for summarization\n",
    "\n",
    "## Recommended Starting Well\n",
    "**Well 5 (NLW-GT-03)** - Most comprehensive documentation, clear tables\n",
    "\"\"\"\n",
    "\n",
    "console.print(Panel(\n",
    "    Markdown(findings),\n",
    "    title=\"ðŸ“Š Exploration Summary\",\n",
    "    expand=False\n",
    "))\n",
    "\n",
    "# Save findings\n",
    "findings_file = OUTPUT_DIR / \"exploration_findings.md\"\n",
    "with open(findings_file, 'w') as f:\n",
    "    f.write(findings)\n",
    "\n",
    "console.print(f\"\\n[green]âœ“ Findings saved to:[/green] {findings_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Action Items\n",
    "\n",
    "Based on this exploration, we should:\n",
    "\n",
    "### Immediate (Day 1):\n",
    "1. âœ… Build document parser with OCR (Docling + RapidOCR)\n",
    "2. âœ… Create table extraction logic\n",
    "3. âœ… Test on Well 5 EOWR report\n",
    "\n",
    "### Short-term (Day 2-3):\n",
    "4. âœ… Build embedding system (nomic-embed-text)\n",
    "5. âœ… Index all wells in ChromaDB\n",
    "6. âœ… Build RAG query system\n",
    "\n",
    "### Medium-term (Week 2):\n",
    "7. âœ… Build structured parameter extractor (Sub-Challenge 2)\n",
    "8. âœ… Format data for NodalAnalysis.py\n",
    "9. âœ… Test on multiple wells\n",
    "\n",
    "### Long-term (Week 3-4):\n",
    "10. âœ… Build agentic workflow (LangGraph)\n",
    "11. âœ… Add vision model (bonus challenge)\n",
    "12. âœ… Optimize and polish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âœ… Exploration Complete!\n",
    "\n",
    "**Key Takeaways:**\n",
    "- Data is well-structured and parseable\n",
    "- Docling works great for this use case\n",
    "- Tables contain the data we need\n",
    "- Ready to start building Sub-Challenge 1!\n",
    "\n",
    "**Next Step:** Begin Day 1 implementation (Document Parser)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
