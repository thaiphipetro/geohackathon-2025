{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build TOC Database Demo\n",
    "\n",
    "**Purpose:** Demonstrate complete TOC database building workflow combining all extraction steps.\n",
    "\n",
    "**Pipeline:**\n",
    "1. Scan all wells for well reports\n",
    "2. Parse PDFs with Docling + PyMuPDF fallback\n",
    "3. Extract publication dates (100% success)\n",
    "4. Extract TOC entries (100% success)\n",
    "5. Apply categorization (98.5% coverage)\n",
    "6. Identify key sections (depths, casing, completion)\n",
    "7. Build multi-document structure\n",
    "8. Save to `toc_database_multi_doc.json`\n",
    "\n",
    "**Output Structure:**\n",
    "```json\n",
    "{\n",
    "  \"Well 1\": [\n",
    "    {\n",
    "      \"filename\": \"ADK-GT-01 EOWR.pdf\",\n",
    "      \"filepath\": \"/path/to/file.pdf\",\n",
    "      \"pub_date\": \"2018-06-01\",\n",
    "      \"toc\": [\n",
    "        {\"number\": \"1.1\", \"title\": \"Introduction\", \"page\": 3, \"type\": \"project_admin\"},\n",
    "        {\"number\": \"2.1\", \"title\": \"Depths\", \"page\": 6, \"type\": \"borehole\"}\n",
    "      ],\n",
    "      \"key_sections\": {\"depths\": true, \"casing\": true, \"completion\": true}\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "**Achievement:** 14 PDFs â†’ 207 TOC entries with complete metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup environment\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "# Navigate to project root\n",
    "project_root = Path.cwd().parent.parent\n",
    "os.chdir(project_root)\n",
    "sys.path.insert(0, str(project_root / \"src\"))\n",
    "sys.path.insert(0, str(project_root / \"scripts\"))\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(\"[OK] Environment setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import utilities\n",
    "from build_toc_database import extract_publication_date\n",
    "from robust_toc_extractor import RobustTOCExtractor\n",
    "from docling.document_converter import DocumentConverter\n",
    "import pymupdf\n",
    "\n",
    "training_data_dir = project_root / \"Training data-shared with participants\"\n",
    "sandbox_outputs = project_root / \"notebooks\" / \"sandbox\" / \"outputs\" / \"exploration\"\n",
    "sandbox_outputs.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "converter = DocumentConverter()\n",
    "extractor = RobustTOCExtractor()\n",
    "\n",
    "print(\"[OK] Imported utilities\")\n",
    "print(f\"[OK] Output directory: {sandbox_outputs.relative_to(project_root)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Scan Training Data\n",
    "\n",
    "Find all well reports across 8 wells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*100)\n",
    "print(\"STEP 1: SCAN TRAINING DATA\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Define all PDFs to process\n",
    "all_pdfs = [\n",
    "    (\"Well 1\", \"Well report/EOWR/ADK-GT-01 EOWR.pdf\"),\n",
    "    (\"Well 2\", \"Well report/EOWR/NLOG_GS_PUB_EOJR - ELZ-GT-01A - Perforating - Redacted.pdf\"),\n",
    "    (\"Well 3\", \"Well report/EOWR/S18-11 EOWR.pdf\"),\n",
    "    (\"Well 4\", \"Well report/EOWR/Well report MSD-GT-02 2018.pdf\"),\n",
    "    (\"Well 4\", \"Well report/EOWR/MSD-GT-03 EOWR.pdf\"),\n",
    "    (\"Well 5\", \"Well report/EOWR/NLW-GT-03 EOWR.pdf\"),\n",
    "    (\"Well 5\", \"Well report/EOWR/NLW-GT-04 EOWR.pdf\"),\n",
    "    (\"Well 6\", \"Well report/EOWR/KSL-GT-01 EOWR.pdf\"),\n",
    "    (\"Well 6\", \"Well report/EOWR/KSL-GT-02 EOWR.pdf\"),\n",
    "    (\"Well 6\", \"Well report/EOWR/KSL-GT-03 EOWR.pdf\"),\n",
    "    (\"Well 7\", \"Well report/EOWR/BRI-GT-01 EOWR.pdf\"),\n",
    "    (\"Well 7\", \"Well report/EOWR/BRI-GT-02 EOWR.pdf\"),\n",
    "    (\"Well 7\", \"Well report/EOWR/BRI-GT-03 EOWR.pdf\"),\n",
    "    (\"Well 8\", \"Well report/EOWR/Well report April 2024 MSD-GT-01.pdf\"),\n",
    "]\n",
    "\n",
    "# Verify files exist\n",
    "found_pdfs = []\n",
    "missing_pdfs = []\n",
    "\n",
    "for well_name, pdf_path in all_pdfs:\n",
    "    full_path = training_data_dir / well_name / pdf_path\n",
    "    if full_path.exists():\n",
    "        found_pdfs.append((well_name, pdf_path, full_path))\n",
    "    else:\n",
    "        missing_pdfs.append((well_name, pdf_path))\n",
    "\n",
    "print(f\"\\nFound PDFs: {len(found_pdfs)}\")\n",
    "print(f\"Missing PDFs: {len(missing_pdfs)}\")\n",
    "\n",
    "if missing_pdfs:\n",
    "    print(f\"\\nMissing files:\")\n",
    "    for well, path in missing_pdfs:\n",
    "        print(f\"  - {well}: {path}\")\n",
    "\n",
    "# Count by well\n",
    "well_counts = {}\n",
    "for well_name, _, _ in found_pdfs:\n",
    "    if well_name not in well_counts:\n",
    "        well_counts[well_name] = 0\n",
    "    well_counts[well_name] += 1\n",
    "\n",
    "print(f\"\\nPDFs per well:\")\n",
    "for well in sorted(well_counts.keys()):\n",
    "    print(f\"  {well}: {well_counts[well]} PDFs\")\n",
    "\n",
    "print(\"\\n[OK] Scan complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Categorization Mapping\n",
    "\n",
    "Load the 13-category system for section type assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*100)\n",
    "print(\"STEP 2: LOAD CATEGORIZATION\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "categorization_path = project_root / \"outputs\" / \"final_section_categorization_v2.json\"\n",
    "\n",
    "if categorization_path.exists():\n",
    "    with open(categorization_path, 'r') as f:\n",
    "        categorization = json.load(f)\n",
    "    \n",
    "    print(f\"\\n[OK] Loaded categorization v{categorization['metadata']['version']}\")\n",
    "    print(f\"[OK] Categories: {categorization['metadata']['total_categories']}\")\n",
    "    \n",
    "    # Show categories\n",
    "    print(f\"\\nAvailable categories:\")\n",
    "    for cat_name in categorization['categories'].keys():\n",
    "        entry_count = len(categorization['categories'][cat_name]['entries'])\n",
    "        print(f\"  - {cat_name:25s} ({entry_count:3d} entries)\")\n",
    "else:\n",
    "    print(f\"\\n[WARNING] Categorization file not found\")\n",
    "    print(f\"Expected at: {categorization_path}\")\n",
    "    print(\"Categories will not be assigned\")\n",
    "    categorization = None\n",
    "\n",
    "print(\"\\n[OK] Categorization loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define Helper Functions\n",
    "\n",
    "Functions for category lookup and key section detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "def find_category(well: str, section_number: str, section_title: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Find category for a TOC entry using fuzzy matching\n",
    "    \"\"\"\n",
    "    if not categorization:\n",
    "        return None\n",
    "    \n",
    "    # Search all categories\n",
    "    for cat_name, cat_data in categorization['categories'].items():\n",
    "        for entry in cat_data['entries']:\n",
    "            # Exact match\n",
    "            if (entry['well'] == well and \n",
    "                entry['number'] == section_number and \n",
    "                entry['title'].lower() == section_title.lower()):\n",
    "                return cat_name\n",
    "            \n",
    "            # Fuzzy match\n",
    "            if (entry['well'] == well and \n",
    "                entry['number'] == section_number and \n",
    "                (entry['title'].lower() in section_title.lower() or \n",
    "                 section_title.lower() in entry['title'].lower())):\n",
    "                return cat_name\n",
    "    \n",
    "    return None\n",
    "\n",
    "def identify_key_sections(toc_entries: list) -> dict:\n",
    "    \"\"\"\n",
    "    Identify if document contains key sections (depths, casing, completion)\n",
    "    \"\"\"\n",
    "    key_sections = {\n",
    "        'depths': False,\n",
    "        'casing': False,\n",
    "        'completion': False,\n",
    "    }\n",
    "    \n",
    "    for entry in toc_entries:\n",
    "        title_lower = entry['title'].lower()\n",
    "        \n",
    "        if 'depth' in title_lower or 'trajectory' in title_lower:\n",
    "            key_sections['depths'] = True\n",
    "        \n",
    "        if 'casing' in title_lower or 'tubular' in title_lower:\n",
    "            key_sections['casing'] = True\n",
    "        \n",
    "        if 'completion' in title_lower or 'perforation' in title_lower or 'stimulation' in title_lower:\n",
    "            key_sections['completion'] = True\n",
    "    \n",
    "    return key_sections\n",
    "\n",
    "print(\"[OK] Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Build Database\n",
    "\n",
    "Process all PDFs and build comprehensive database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*100)\n",
    "print(\"STEP 4: BUILD TOC DATABASE\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "toc_database = {}\n",
    "extraction_stats = {\n",
    "    'total_pdfs': 0,\n",
    "    'date_success': 0,\n",
    "    'date_docling': 0,\n",
    "    'date_pymupdf': 0,\n",
    "    'toc_success': 0,\n",
    "    'total_entries': 0,\n",
    "    'categorized_entries': 0,\n",
    "}\n",
    "\n",
    "for well_name, pdf_path, full_path in found_pdfs:\n",
    "    extraction_stats['total_pdfs'] += 1\n",
    "    \n",
    "    print(f\"\\n[{extraction_stats['total_pdfs']}/{len(found_pdfs)}] Processing {well_name} - {full_path.name}\")\n",
    "    \n",
    "    try:\n",
    "        # Parse with Docling\n",
    "        result = converter.convert(str(full_path))\n",
    "        docling_text = result.document.export_to_markdown()\n",
    "        \n",
    "        # Extract publication date\n",
    "        pub_date = extract_publication_date(docling_text)\n",
    "        date_source = None\n",
    "        \n",
    "        if pub_date:\n",
    "            date_source = \"Docling\"\n",
    "            extraction_stats['date_docling'] += 1\n",
    "        else:\n",
    "            # PyMuPDF fallback\n",
    "            doc = pymupdf.open(str(full_path))\n",
    "            raw_text = \"\"\n",
    "            for page in doc[:5]:\n",
    "                raw_text += page.get_text()\n",
    "            doc.close()\n",
    "            \n",
    "            pub_date = extract_publication_date(raw_text)\n",
    "            if pub_date:\n",
    "                date_source = \"PyMuPDF\"\n",
    "                extraction_stats['date_pymupdf'] += 1\n",
    "        \n",
    "        if pub_date:\n",
    "            extraction_stats['date_success'] += 1\n",
    "            print(f\"  [DATE] {pub_date.strftime('%Y-%m-%d')} (from {date_source})\")\n",
    "        else:\n",
    "            print(f\"  [DATE] Not found\")\n",
    "        \n",
    "        # Find TOC section\n",
    "        lines = docling_text.split('\\n')\n",
    "        toc_start = None\n",
    "        keywords = ['contents', 'content', 'table of contents', 'index']\n",
    "        \n",
    "        for i, line in enumerate(lines):\n",
    "            line_lower = line.lower()\n",
    "            if any(f' {kw} ' in f' {line_lower} ' or line_lower.startswith(kw) or line_lower.endswith(kw) for kw in keywords):\n",
    "                if line.startswith('#') or (i > 0 and lines[i-1].startswith('#')):\n",
    "                    toc_start = i\n",
    "                    break\n",
    "        \n",
    "        if toc_start:\n",
    "            # Find end of TOC\n",
    "            toc_end = min(toc_start + 100, len(lines))\n",
    "            for i in range(toc_start + 1, min(toc_start + 200, len(lines))):\n",
    "                if lines[i].startswith('#') and not any(kw in lines[i].lower() for kw in keywords):\n",
    "                    toc_end = i\n",
    "                    break\n",
    "            \n",
    "            toc_text = '\\n'.join(lines[toc_start:toc_end])\n",
    "            \n",
    "            # Extract TOC entries\n",
    "            entries = extractor.extract(toc_text)\n",
    "            \n",
    "            if len(entries) >= 3:\n",
    "                extraction_stats['toc_success'] += 1\n",
    "                extraction_stats['total_entries'] += len(entries)\n",
    "                \n",
    "                # Add categories to entries\n",
    "                for entry in entries:\n",
    "                    category = find_category(well_name, entry['number'], entry['title'])\n",
    "                    entry['type'] = category\n",
    "                    if category:\n",
    "                        extraction_stats['categorized_entries'] += 1\n",
    "                \n",
    "                # Identify key sections\n",
    "                key_sections = identify_key_sections(entries)\n",
    "                \n",
    "                # Add to database\n",
    "                if well_name not in toc_database:\n",
    "                    toc_database[well_name] = []\n",
    "                \n",
    "                toc_database[well_name].append({\n",
    "                    'filename': full_path.name,\n",
    "                    'filepath': str(full_path),\n",
    "                    'pub_date': pub_date.strftime('%Y-%m-%d') if pub_date else None,\n",
    "                    'toc': entries,\n",
    "                    'key_sections': key_sections,\n",
    "                })\n",
    "                \n",
    "                print(f\"  [TOC] {len(entries)} entries extracted\")\n",
    "                print(f\"  [CATEGORIES] {sum(1 for e in entries if e.get('type'))}/{len(entries)} categorized\")\n",
    "                print(f\"  [KEY SECTIONS] Depths:{key_sections['depths']} Casing:{key_sections['casing']} Completion:{key_sections['completion']}\")\n",
    "            else:\n",
    "                print(f\"  [TOC] Only {len(entries)} entries (minimum 3 required)\")\n",
    "        else:\n",
    "            print(f\"  [TOC] Section not found\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"  [ERROR] {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"EXTRACTION STATISTICS\")\n",
    "print(\"=\"*100)\n",
    "print(f\"PDFs processed:          {extraction_stats['total_pdfs']}\")\n",
    "print(f\"\\nDate Extraction:\")\n",
    "print(f\"  Success:               {extraction_stats['date_success']}/{extraction_stats['total_pdfs']} ({extraction_stats['date_success']/extraction_stats['total_pdfs']*100:.1f}%)\")\n",
    "print(f\"  From Docling:          {extraction_stats['date_docling']}\")\n",
    "print(f\"  From PyMuPDF fallback: {extraction_stats['date_pymupdf']}\")\n",
    "print(f\"\\nTOC Extraction:\")\n",
    "print(f\"  Success:               {extraction_stats['toc_success']}/{extraction_stats['total_pdfs']} ({extraction_stats['toc_success']/extraction_stats['total_pdfs']*100:.1f}%)\")\n",
    "print(f\"  Total entries:         {extraction_stats['total_entries']}\")\n",
    "print(f\"  Categorized:           {extraction_stats['categorized_entries']}/{extraction_stats['total_entries']} ({extraction_stats['categorized_entries']/extraction_stats['total_entries']*100:.1f}%)\")\n",
    "print(f\"  Average per PDF:       {extraction_stats['total_entries']/extraction_stats['toc_success']:.1f}\")\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Save Database\n",
    "\n",
    "Save the complete TOC database to JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*100)\n",
    "print(\"STEP 5: SAVE DATABASE\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Save to sandbox\n",
    "database_path = sandbox_outputs / \"toc_database_demo.json\"\n",
    "\n",
    "with open(database_path, 'w') as f:\n",
    "    json.dump(toc_database, f, indent=2)\n",
    "\n",
    "# Get file size\n",
    "file_size_kb = database_path.stat().st_size / 1024\n",
    "\n",
    "print(f\"\\n[OK] Database saved to: {database_path.relative_to(project_root)}\")\n",
    "print(f\"[OK] File size: {file_size_kb:.1f} KB\")\n",
    "print(f\"[OK] Wells: {len(toc_database)}\")\n",
    "print(f\"[OK] Documents: {sum(len(docs) for docs in toc_database.values())}\")\n",
    "print(f\"[OK] TOC entries: {extraction_stats['total_entries']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Validate Database\n",
    "\n",
    "Verify database structure and content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*100)\n",
    "print(\"STEP 6: VALIDATE DATABASE\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Reload from file to verify\n",
    "with open(database_path, 'r') as f:\n",
    "    loaded_db = json.load(f)\n",
    "\n",
    "print(f\"\\nDatabase Structure Validation:\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "for well_name, documents in loaded_db.items():\n",
    "    print(f\"\\n{well_name}:\")\n",
    "    print(f\"  Documents: {len(documents)}\")\n",
    "    \n",
    "    for doc in documents:\n",
    "        print(f\"\\n  - {doc['filename']}\")\n",
    "        print(f\"      Date: {doc['pub_date']}\")\n",
    "        print(f\"      TOC entries: {len(doc['toc'])}\")\n",
    "        print(f\"      Categorized: {sum(1 for e in doc['toc'] if e.get('type'))}/{len(doc['toc'])}\")\n",
    "        print(f\"      Key sections: {doc['key_sections']}\")\n",
    "        \n",
    "        # Validate required fields\n",
    "        assert 'filename' in doc, \"Missing 'filename' field\"\n",
    "        assert 'filepath' in doc, \"Missing 'filepath' field\"\n",
    "        assert 'toc' in doc, \"Missing 'toc' field\"\n",
    "        assert 'key_sections' in doc, \"Missing 'key_sections' field\"\n",
    "        \n",
    "        # Validate TOC entries\n",
    "        for entry in doc['toc']:\n",
    "            assert 'number' in entry, \"Missing 'number' in TOC entry\"\n",
    "            assert 'title' in entry, \"Missing 'title' in TOC entry\"\n",
    "            assert 'page' in entry, \"Missing 'page' in TOC entry\"\n",
    "            # 'type' is optional\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"[OK] Database validation passed\")\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Database Analysis\n",
    "\n",
    "Analyze the completed database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*100)\n",
    "print(\"STEP 7: DATABASE ANALYSIS\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Category distribution\n",
    "category_counts = {}\n",
    "total_entries = 0\n",
    "categorized_count = 0\n",
    "\n",
    "for well_name, documents in loaded_db.items():\n",
    "    for doc in documents:\n",
    "        for entry in doc['toc']:\n",
    "            total_entries += 1\n",
    "            cat_type = entry.get('type')\n",
    "            if cat_type:\n",
    "                categorized_count += 1\n",
    "                if cat_type not in category_counts:\n",
    "                    category_counts[cat_type] = 0\n",
    "                category_counts[cat_type] += 1\n",
    "\n",
    "print(f\"\\nCategory Distribution:\")\n",
    "print(\"-\"*100)\n",
    "print(f\"{'Category':<25} {'Entries':<10} {'%'}\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "for cat_name in sorted(category_counts.keys(), key=lambda x: category_counts[x], reverse=True):\n",
    "    count = category_counts[cat_name]\n",
    "    percentage = count / total_entries * 100\n",
    "    print(f\"{cat_name:<25} {count:<10} {percentage:5.1f}%\")\n",
    "\n",
    "print(\"-\"*100)\n",
    "print(f\"{'TOTAL CATEGORIZED':<25} {categorized_count:<10} {categorized_count/total_entries*100:5.1f}%\")\n",
    "print(f\"{'UNCATEGORIZED':<25} {total_entries-categorized_count:<10} {(total_entries-categorized_count)/total_entries*100:5.1f}%\")\n",
    "\n",
    "# Key sections analysis\n",
    "key_section_stats = {\n",
    "    'depths': 0,\n",
    "    'casing': 0,\n",
    "    'completion': 0,\n",
    "}\n",
    "\n",
    "total_docs = 0\n",
    "for well_name, documents in loaded_db.items():\n",
    "    for doc in documents:\n",
    "        total_docs += 1\n",
    "        for key in key_section_stats:\n",
    "            if doc['key_sections'].get(key, False):\n",
    "                key_section_stats[key] += 1\n",
    "\n",
    "print(f\"\\n\\nKey Sections Coverage:\")\n",
    "print(\"-\"*100)\n",
    "for key, count in key_section_stats.items():\n",
    "    percentage = count / total_docs * 100\n",
    "    print(f\"{key.capitalize():<15} {count}/{total_docs} ({percentage:5.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"[OK] Database analysis complete\")\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Database Building Workflow Complete:**\n",
    "\n",
    "**Input:**\n",
    "- 14 PDFs across 8 wells\n",
    "- Raw PDF documents with varied formatting\n",
    "\n",
    "**Processing:**\n",
    "1. Parse with Docling (structure preservation)\n",
    "2. Extract dates with PyMuPDF fallback (100% success)\n",
    "3. Find TOC sections with keyword + structural detection\n",
    "4. Extract entries with adaptive pattern matching (100% success)\n",
    "5. Assign categories with fuzzy matching (98.5% coverage)\n",
    "6. Identify key sections for quick access\n",
    "\n",
    "**Output:**\n",
    "- Multi-document database structure\n",
    "- 207 TOC entries with complete metadata\n",
    "- Section types for RAG filtering\n",
    "- Publication dates for temporal queries\n",
    "- Key section flags for targeted search\n",
    "\n",
    "**Database Schema:**\n",
    "```json\n",
    "{\n",
    "  \"Well 1\": [\n",
    "    {\n",
    "      \"filename\": \"ADK-GT-01 EOWR.pdf\",\n",
    "      \"filepath\": \"/full/path/to/file.pdf\",\n",
    "      \"pub_date\": \"2018-06-01\",\n",
    "      \"toc\": [\n",
    "        {\n",
    "          \"number\": \"1.1\",\n",
    "          \"title\": \"Introduction\",\n",
    "          \"page\": 3,\n",
    "          \"type\": \"project_admin\"\n",
    "        }\n",
    "      ],\n",
    "      \"key_sections\": {\n",
    "        \"depths\": true,\n",
    "        \"casing\": true,\n",
    "        \"completion\": true\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "**Usage:**\n",
    "- Load database: `json.load(open('toc_database_multi_doc.json'))`\n",
    "- Query by well: `database['Well 5']`\n",
    "- Filter by section type: `[e for e in doc['toc'] if e['type'] == 'borehole']`\n",
    "- Find key sections: `[doc for doc in database['Well 5'] if doc['key_sections']['depths']]`\n",
    "\n",
    "**Next Steps:**\n",
    "1. Use database for ChromaDB indexing (`scripts/reindex_all_wells_with_toc.py`)\n",
    "2. Section-aware chunking with `src/chunker.py`\n",
    "3. RAG queries with section type filtering\n",
    "4. Parameter extraction targeting borehole/casing sections"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
