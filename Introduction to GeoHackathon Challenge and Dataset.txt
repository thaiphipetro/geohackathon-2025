Hello everyone. Welcome to another session of the boot camp for the Jewaton uh 2025. Uh it's my pleasure to host you today. My name is Leila Hashimi. I am a scientist at TNO Dutch Research Center in the Netherlands. And uh today we have our presenter uh Dr. Pman Shrani from Osno uh he will present uh to us uh today on uh introduction to the Johakaton challenge and the data sets. But just uh before uh starting the uh today uh session, I wanted also to take the uh the time to uh thank our sponsors uh who who who are supporting us in this uh Johathon and also thank you for joining us and I think it's going to be one of the important uh boot camp that you are going to learn about the challenge that you you will be involved and the data set that will be shared with you. Just before uh before uh giving the floors to our speakers, I wanted to just uh remind you that you will be muted during the the session. But please uh post your question in the chat box and we will address them at the end of the sessions and if there is any further more questions and discussion please uh bring it to the uh discord so we can just follow up there. So without further ado, I would like to invite our speakers and give the floors to you Benjamin. Thank you very much Leila. Good day everyone. Um I'll first find the button to share my screen. [Music] I hope you can see the screen in the full screen mode. Yes. Okay, perfect. Um well great that you joined uh today um my name is uh Pes mani as Leila introduced I'm also working at TNO Dutch research institute for applied scientists applied sciences um I'm working there as a senior scientist mainly focusing on AI and digitalization of the energy systems and today I will uh present to you the ideas behind this um geo hackathon this year. Um I don't have a long presentation but we wanted to give you sufficient time in case you have any question so that we can address them uh live today and of course this is not the only chance for the for the people who are uh watching the recordings u we would always be available on the discord channel to address your questions and in case there are a lot of questions and doubts still available we might plan a Q&A um somewhere at the end of all the boot camp sessions. So that's that's also for you. Good to know. Uh make sure that all your questions will be on on discord and u in case of any doubt feel free to reach out to us. So for this year hackathon um in 2025 of course with all the SP sections that are collaborating for uh putting this uh challenge together with two uh technical sections. to geothermal technical sections and the data science and engineering analytics technical sections. We put this challenge together which well in short what we would uh like you to do is this picture. So we have quite a lot of um uh well reports and I I'll first give you a summary of the hackathon and then I'll go into every element of the data why we have this hackathon and also some description about the criteria. Um so we have quite a lot of well reports are often documented in a in a in a report which are well scanned in PDF. The most recent data are quite uh standardized in terms of the format of how these well reports are delivered. But going into the older documents, of course, you also see some handwritten more uh more let's say um uh ad hoc type of uh structure for this for this files. And um within this weld reports there are quite a lot of important information. So of course you what you will see about the whole general process of the drilling of the well uh the trajectory the completion of the well. So that would uh give information about the um the different sections u of the of the of the well its diameter the depth and also it will give you some information about the the inclination and the dog leg and all the things that you would need from the well. But next to that also often as well after they've been drilled there will be samples taken from them so that you can get some fluid characterization which are known as the PBT reports or flute characterization reports. Um and well sometimes you have some well tests so that will tell you how good your well is actually uh performing. So how much production you can get at a certain pressure that you impose into the system. And if your wells are also equipped with some equipment sometime also you have this information about those. So for instance the most common one will be the artificial lift systems. If there is an electrical submersible pump in the in the well you also have some additional files that will explain the the type of the pump the curves of the pump and things like that. So there are already quite a lot of different reports and uh what we wanted to do this uh this time is that um one of the well in my opinion the added value of AI is not just about making a predictive model or a generative model but it's also about automating some of the existing workflows that we have in the in the subsurface application and this is an example of that. So what we would like you to do is to create an a aentic AI workflow that automate the process of getting the information from the well reports fitting it into a model that it would also be supplied to you and in the end you can um specify the the well performance. So more or less for a given pressure that you apply to the system what sort of flow rate you'll be able to uh produce from this uh from this resolution there will be a detailed presentation about the wellbo performance and vertical lift performance calculation given I believe next uh week um so that will be a very good boot camp to attend especially for those who are not familiar with this process of predicting the world performance So in a nutshell this is the idea right. So an agentic workflow being able to extract the relevant information from your web reports and then would estimate what will be your web performance. So of course before deep dive into the into the into the challenge why this challenge is important. So also a little bit of a context of why we decided to to put this challenge together. And the idea is that especially if if you go to an oil and gas or a geothermal company, they have a portfolio of the wells that have been drilled, a lot of technical documents that comes with it. And um one of the thing that we see is that first of all, there's quite a lot of hours and budget goes into into uh analyzing this u uh reports. And if you especially do it manual, the thing is that um you won't be able to proactively identify some issues that might encounter in several let's say drilling campaign or in several completion. If you have an experienced person who has observed all this drilling and completion work, well they will be able to notice but of course that would also be limited only to the observation that they have received. But in a larger company where you get like thousands of wells being drilled and completed and all those reports are available, if you want to have a systematic learning on those sort of data, it will be very difficult to do it on a manual basis. So that would bring quite a lot of added value and costsaving to the energy companies to automate the processing of these documents. And if you talk about an individual well, so an a well has been drilled and you want to do some analysis with respect to how much it can perform uh how good this well can perform or come up with an operational strategy or come up with an assessment on the integrity of the wells. All of these things together if you have this workflows automated again you could also save quite a lot of cost from the operators and the engineers who are responsible for those well. So a lot of added value already just by automating this workflow of extracting the information and then being able to plug it into the models or the workflows that you want to use. So um the work itself that we have defined is is pretty challenging. Okay. So what we did is that we define three sub challenges. So participants well we encourage you to finish all three top challenges but you can also say oh we only manage the first or the second sub challenge and you would still be graded for this and I will explain to you how the the grading will work. So the three sub challenges we define it in a way that also gives you a little bit of a direction how you should think about hacking this problem. So the first one is about creating a retrieval augmented generation workflow, a rag workflow that can create an accurate and complete summary of this well completion report within a given amount of words based on the user prompt. This is quite detailed of course how we put it there because we wanted to make it very specific and later on it comes also into the criteria that we use for the grading. But as a first step, we simply will say well we don't want you to do any calculation. We don't want you to uh um set up an agentic workflow but simply if you be able to extract the right information from this reports and bring it to the user based on the prompt that they would give that would already account for 50% of the grades that we would give to the attendees. And I think this is especially with the uh boot camp that was provided. There will be another boot camp also at the end of the boot camp series and with a lot of tools and um manuals that are available online. I think this will be a a a step that most of the attendees will be able to to achieve this. And for this reason to make sure that the barrier to enter the boot camp hackathon is also low we uh put 50% of the of the total grade on the first task. So this is just a a summary and then the judges will design some prompt and then ask few questions based on the documents that we have and then they're going to see how accurate those information will be retrieved from the from the well reports that we have. So this is the first sub challenge. The second sub challenge is now getting closer to the to the to the agentic workflow. So after being able to summarize the information, we would like to use the same workflow that we have developed, but this time to retrieve the retrieve the parameters that we need in order to run the scripts that we will provide to you for the well production capacity and model analysis. So this is a step that you say okay now I can summarize the report for you but are you able to also meaningfully and accurately extract the parameters that I need in order to run my wellboard model. So this will be the second um challenge that we will define and then the last challenge is now the agentic part where we will say okay well you have the rag application we will provide the script to you if you've managed to do the second one now the idea will be that how this agent will understand to plug this inputs that you need from the script first retrieve it from the document and then put it into the into the code to the script that we will give you for the web performance and then show the results. That will be the the last part of this. So actually agentic part will be the only the third sub challenge that we have. Again, I encourage even if you manage to only do half of this, whatever uh level that you reach, um please submit your u submit your uh your work because maybe you've done a great job or in the first step and that already uh would give you in in a winning position. So uh um make make sure that you know uh even if you're partially done submit your work. Um for those who are a little bit more into extra challenge and we never say no to extra challenge, we also put a bonus challenge and this is uh to now not only extract the information from the tables and the text data that will be available in the in the well reports, but some of the information that we have in the well reports are also in the images. And we say well we would give a bonus if someone also use some vision models in order to extract the right information for running the web performance from the images that will be in the web course. So that will be the bonus challenge that we will have for you. If you want to just do the the main sub challenges just by going through the tables, going through the text that is available, you'll be able to to get quite a lot of uh u uh information already that can be used in order to mark the models. Um where do we get the data from? So the data is coming from the Dutch subsurface portal or Dutch oil and gas portal analog.nl. And here there's quite a lot of information and data available on the wells that have been drilled and produced or even abandoned in the Netherlands. Uh, of course including the well and drilling reports. So there are available if you're interested to well look for more files from this database. Well, feel free to do so. There will be more than thousand wells that you can find information over there. Uh to make your life easy, we've already um downloaded a selection of around 8 to 10 wells with a full list of the documents that are available there. So you don't have to go through the end log and uh download the files for yourself. But if someone is open for a challenge and want to download more reports to make sure how good their workflow works, you know, be our guest. There is no problem. And uh for the same reason also we put the website here that you can actually go and if you want get more information and data from this um you'll be receiving this set of um data from the wells as I said between 8 to 10 uh wells with all the reports will be sent to you and we also will keep uh some wells for the juries and those information you will not see. You can go to end log and download some additional wells and maybe you're lucky and exactly the well that you're testing the workflow is the one that also the jurist will see. Well, that we have no control on that. But um from the same database we also have two um few let's say few candidates for the juries in order to run the workflow and see how good your workflow works. So juries will not u evaluate your u submission based on the data that we sent to you. They would evaluate it based on a different data that is available. Um this just give you a little bit of a um flavor of what sort of data you will be able to see in this world report and it's quite different. So for this is one case that this is actually a part of your data set that you would receive that provide this information from the completion and of course everything is already here available with respect to the uh the depth of the well the true vertical depth the measured depth the diameter um the um outer diameter of the pipe the inner diameter of the pipe. So this is actually in a form of a table or in some cases actually directly you'll get the table like this in the in the report that this is the main information that we need to extract in order to run the script. Um for the bonus challenge as I said we also have like this sort of a graphical uh representation which are showing the world trajectory and uh if for the bonus question someone is going to actually extract information from this again that's also will be great and you want to get additional uh points for for doing this uh extra assignments. So I would just quickly u open some of this reports for you. Uh and then I will go back again to the to the presentation. So this is an example of a well that you would receive. Yeah. So this is a geothermal well drilled in the Netherlands. Quite a lot of information of course about the date who has been responsible. Then you have this sort of let's say u um the trajectory information from the well that has been drilled and going down then you also see something about the casing information. Um in this case there are two wells. So keep in mind that maybe sometimes you need to specify the well. Yeah. So that is it a GT1 or GT2 or maybe you need to make your rag application a little bit more intelligent that it identifies that within this report there are two so then it asks you do you want GT1 or GT2 that's up to how you want to design this but please also be careful that especially a lot of geothermal projects because there are two wells instead of one well you would also sometimes get one report that contains information for both wells. So that um that's also something to to keep in mind. And then we have this well diagram. Again this information already from this text you can you can already extract but maybe again someone wants to get the whole image and do something with this. If someone does that even though it's part of the first challen challenge but if someone again does it also with the vision models or the vision LLM or generative models or whatever this uh optical recognition algorithms then not only they will get the grade for the first um sub challenge but also they will get some bonus uh points for that and yeah this would be the kind of a typical information that you receive from the well report. This of course is one of the example. Um, oh this is the one that for some of the wells you would also get some fluid properties gas for interrupting. Uh, someone is asking can you please zoom in a little bit so they can see it better. Yeah. Yeah, that's good. Thanks for letting me know. Uh yeah, I'll go to the another report and then I'll go do it a bit more zoom in. Remove this so you should be able to see this better. Um so this is again another well report that we have. Um here again you find information about the well. It goes down. You see the same type of trajectory information. I think this was for the for the vision question that we asked. Here you have the information about the casing, tubular, cementing, all the things that are there. You won't need all of this, right? So you don't need this uh volume and the weight of the slurry is and you know all the cementing job. You don't need this but of course this will be in my opinion this will be one of the most uh important tables that you should get the information from right. to the dep ID and OD of the diameters and well not to repeat myself again if you do this also based on the based on the uh um region models you will get an additional point. Yeah. So this is this will be the the part that you're going to see. Um of course here sometimes you see in these walls that there are several of this. So sometimes there's information also about the wellhead or about the cementing. So um we would not tell you what sort of reports we're going to choose for the evaluation but what we're going to do is that we're going to have one report which is more straightforward as information about one well and then we try to do how your workflow works there. We would also have a set which will be more challenging maybe even with some information that are handwritten or you know a little bit also multilingual. So and then we want to see how robust will be your workflow. But if you manage to at least test your workflow based on the data that will provide to you I think you will have a quite a good chance. Um this is a little bit more elaborate uh well report also you know you see all the logs available you don't really need this right so we didn't want to cut the reports because also for you it's a good education to go through this but well expect that some of these well reports has more information than you need and I already gave you hints on where you need to look for the information that are out there and the last one that I wanted to show to you because in some cases we also have this report which we will share it with And over here you have the data which are actually quite nicely structured. So you want your measure depth, you want your TVD uh and this actually would directly give you that information in a very nice table structure. Um so that's again another flavor of the data that you would receive. So um I just selected few that I thought their reporting style are a little bit different than each other. you will receive this information. Again, if there is anything missing, if there is any question that you have about the data, feel free to uh ask it on this court and we'll be happy to take your question. So that's uh some example on the data and the data structure that you're going to receive. And what do you need for the sub challenge is of course the nodal analysis. So we want you to extract this. So first step was the summarization. Second step was to extract the data that you need in order to run the script for the valable performance. And then the third step is this agentic step where we will ask you to actually plug this information from the report to the to the script and then run it. Uh we would give you this script. So pretty simple uh Python file. This is uh one of the codes that we extracted from our Gemini digital twin platform. This is a digital twin platform we develop at TNO for geothermal systems. Um next year early next year it will be open source and there is a boot camp about it also end of end of this week. Um so you would get access to a script from the from Gemini digital twin that is capable of doing a nodal analysis and nodal analysis is this pressure flow analysis that you use in order to calculate the wel performance. uh I I picked the the top part of the uh script. So because I need to I thought it's nice to explain few things to you to make your life easy. We already hardcoded some parameters because otherwise in order to know what is the fluid uh uh properties, what sort of reservoir pressure do you have? Things like this you actually need to uh extract more information from other documents. And we thought for a short period that we are for the hackathon this is just not feasible. So we've hardcoded the parameters for you. You don't have to touch this. Just keep it as it is. Uh if you make changes here, please make sure that you have uh commented nicely and clearly in the in the code that you sent to us. So you know for inance maybe you say from the well reports we already extracted in some cases a functionality that would know the the ESP depth the pump depth and we also put that in the workflow but then make sure that is already commented so that we we understand the difference of your code with the others. So things like the fluid properties, the roughness of the pipe, reservoir pressure, valid pressure, the uh productivity index. So um the connection between reservoir and the well and the depth of the pump. This is already um hardcoded and also we have included a hardcoded uh pump curve so that if you have a downhole pump at a depth of 500 what sort of curve parameters you have over there. So more or less don't touch this. The only thing that you would need is of course is this structure of the well. So when you extract the information from this reports, it's very important that you uh put it in the same structure as we have in this code because in this way you'll be able to run the code. So in the second challenge, the sub challenge, you should be able to extract this information, measure depth through vertical depth and the inner diameter of the pipe. And then the agenting workflow when it extract this information or if something is missing, it needs to interact. But when the agent realize that everything is is uh complete, it needs to create a structure that looks like this and then plug this structure into the code and then the code will be able to run. Right? So if it has this structure it will run. If it has other type of structures then it will be difficult to to run the model. There might be cases that actually you will have no solution. We have also a kind of a a check in the code that sometimes maybe physically you don't have any solution. Maybe because we have hardcoded the reservoir pressure or depth of the pump you don't have a solution but that is absolutely okay. Right. So this this is just something that if you see oh there is no solution doesn't mean that your code is wrong it could be that physically it's not possible but if the structure is not given in the right way the code will not run so you would already get error at very early stage in your uh in your analysis. So very important is the formatting of the well completion data. This is the most important thing in order to set up your agent. And am I missing anything? No. So there is as I said this there's a boot camp on 22nd of October that would explain to you the basics of the uh this nodal analysis and the performance analysis. So a lot of background information I don't talk about it because you'll get it that come and you will all receive a python code with this uh with this to perform this model analysis. Okay, we put some constraints. This also has been already in the website. Just for the sake of completion, I also included here. So, um we won't be able to guarantee that all the juries will have access to GPU. So, for this reason, we suggest that you focus on smaller models. you know few hundreds million parameters would be okay but going to the billion parameters would make it very difficult for the jurists to evaluate this. Um so the key here will be to focus more on lighter model to to get this up and running. Um we asked you to do it in do your developments in uh in Python uh and also the packages is also easily and uh freely can be installed using uh p install or cond that's also accepted. Um and then if you would like to set up a new environment and then there will be some dependencies between the different libraries that you want to have a requirement file as well then please also include it in your in your uh submission. Uh we would like the codes to have some level of um description as well. So make sure that if you're doing anything in the code you put some comments explaining the thing behind it. Especially also talked about this hardcoded part. If you want to change that then please make sure that's also in your in the in the comments of your code and the most important thing is about the use of open source models right so we we we have some preference that we also put it here um so for instance for the large language model if you have the possibilities to use and through lama you can get access to a lot of uh open-source large language models but that's preferred but you're not limited into that. Again, if you just say we use this uh libraries the requirement and this is the let's say the the environment that has to be set up then we will also take care of that. um for the open source part because there are also questions on on discord uh well whether you do uh lang chain or any other thing that's up to you as long as we can use it and you know there is no license cost associated with the tool that you use and it's open source that's absolutely fine um also in the past we were approached for instance that some tools like N8N and for the agents uh would enable you that with some well free trials set up an agent. Uh the issue with using those tools is that then if you have a jury that needs to look into 20 30 different submission we cannot guarantee that just by a trial license that will be possible. So trial license is also not uh eligible in our in our hackathon. So just use open source tool. There are lots of things that you can use and also there will be all these boot camps that uh would already tell you how you can use the existing tools in order to set up your own agent. This for you to know a little bit how we are going to uh grade the different submission. So what's important for us is that well to a given prompt or question that we ask how accurate the response is for summarization getting the data and also the output that we would expect from the from the uh from the agent workflow. The speed of the response is also important and each jury will do it based on their own computer that they have access to or cluster they have access to. uh but yeah they will be able to differentiate between which one was faster and which one was not. And of course the number of prompts required to get to the final answer is also important. This doesn't mean that well um if you can do it in the first prompt that's great. No we also would look a little bit the interactivity. So sometimes maybe in the prom we intentionally will uh will not include one parameter and then see if the if the agent will also interact into this. So number of prompts in order to get to the final result that's something that we think that how we're going to grade this the three sub challenges that we have. So we have 50% for the first sub challenge. So just that summarization um step that is based on the rag that will be 50% of the grade. The second sub challenge which is getting this data in a right format that can run the model is 20%. And then 30% will be for the for the final sub challenge which is this agentic workflow in order to also put this parameters into the code and run it. Yeah. Um the bonus challenges uh is similar to to the uh sub challenge three. So as said if someone will do the summarization already by also getting some of this uh vision models uh that already is a pretty uh good grade right so that already you can already focus quite a lot on the first one and I think uh you'll be able to get a good score that uh I believe this was my last style if I'm not mistaken. Yes. So that was my final slide. I think I've uh tried to give a bit of a view on how we come up came up with this uh with this challenge. Um yeah, if there are questions, I think there already few in the chat. Uh so uh Leila feel free to shoot them. Great. Thank you very much Benjmon for introducing the challenge and also walking us through the data set and some examples. I believe it was quite useful and informative already. Uh and there are indeed some questions in the chat box. Uh you have already addressed some of them but just to make sure if there is anything extra you wanted to add I will just read it for you and just yeah please give us some more information if needed. The first question uh actually we received it in the uh discord channel. Can we implement using langraph for agent development as opposed to lang chain? Is there a constraint around this? Yeah. So indeed that's that's uh as also tried to address during the presentation. Um no. So it's absolutely okay to use langraph as well. Not the the the platform because I mean they also come with some platforms that is uh paid. So if you just use the open source model for that that's that's no problem right. So lang chain lang graph uh we simply don't care. So uh each of them will be fine. Yes. Very good. The next question uh regarding the first channel a challenge the summary. Can we be more specific? You just say there is a lot of information that is not useful. [Music] Maybe I don't know if the question is clear to you. Yeah. No it's clear and that's a very good question. So the part of the information that are not useful um so I showed you a few of the example and one of the example you saw it was a much larger PDF with all these logs that are available at the end of it and those are images that we don't expect you to process them but what we would ask the question is that simply will be uh how many wells are reported in this well can you specify the name of the wells that have been drilled can you find the location of the well that has to be real. So the the text part that is in the well report, we expect that your rack workflow has a good uh potential to summarize those information. Yeah, extracting the information from the table that's that's also very critical because that's an important thing for the challenge two and three. So this also will be part of the question that we would ask but expect that in the first uh uh summary part the type of the question that jury will ask is about the data that you can find in the text of the web reports. If there is still more question on this topic feel free to type it in the chat. Yeah please I would like also to invite the participants. I've seen already the number of participant increased uh from the beginning. So please if you missed some part of the presentation watch the uh the recorded uh video uh afterwards and if there is any question just please uh raise it up here or later on in the discord channel. But the next one in the chat box we have still some more questions. Um you already addressed it in somewhat uh on what parameters efficiency of the model will be judged the efficiency of the model like to be more specific. So more or less this this will how the we would look into the um this these are the main criteria we're going to use for the for the grading. Yeah. So accuracy of the response. So this is very simple right? So we ask for the give me the true vertical depth of the GT0 6 well and then it should give an answer when we're going to validate if it gives the right answer or not. So this will be the part of the accuracy that we're going to we're going to evaluate the speed of the response. So here of course the speed is an important one because if you be able to provide an accurate answer using lighter models well there you have a quite an advantage compared to the other participant. So that's why speed is also very important. And then the last one as we said is the number of prompts right. So here again we don't have a hard criteria of more or less but well we would expect that okay within a certain number of responses we get there when we design a very specific prompt and we ask it to the agent we expect that more or less in the next uh iteration we get an answer. If a specifically we leave an important information out and we want the agent to interact with us then of course that is also something we would expect to see. So the number of prompts would be uh related to the to the prompt that we actually give and we're going to use the same prompt in order to evaluate all these workflows against each other. So in that sense it would be a fair comparison on uh uh if the agent just didn't understand that something important is missing and still give an answer or it ask you to provide a clarification. So that's that will be the what we're going to look into it be performance if you mean by uh speed. So that's that's mainly what I said about the lighter model will be able to still give an accurate result that's or accurate respond to the to the question to the prompt. Very clear. Uh one other question about APIs any APIs provided other than Gemini twin or everything required is open source. you already mentioned about this open source models needs to be used but maybe good to emphasize again so as I said for the agentic workflow for the rag workflows that you're going to put forward you have to use the open source for the um so when we say Gemini please do not mix it up with the Google Gemini right so when we say Gemini Gemini is the name of our digital twin platform and that will be that will be it open source uh beginning of next year but we would already give you the script and the code that you need in order to do the well for performance analysis. So this is something that you don't need to code it yourself. We provide it to you. We also give you the structure of how your agent need to uh provide the information to the script in order to run the model. I hope this clarifies but uh do not send submissions with the Gemini API. That's not accepted. You're very good actually to mention that. Yeah. No, it was a good question because maybe the Gemini example then people start to create API with Gemini. No, that's that's not the that's not the goal of this assignment. Yeah, very good. Actually, I just grouped the other question related to API because it was already addressed. One more question about the timing like the schedule when will the data sets and the Python code for nodal analysis will be released? Yeah, that's a good one. So, um I need to what? You need to help me Leila with this. Do we do at the end of the boot camp or already do we release it this week because I think I Yeah, sorry. Yeah, I think yeah, we do have them available. Yeah, we can start just releasing this week. The the data is available and usually in the previous uh hackathons and the week that we had the introduction to hackathon, we released the data. So already some participants can start play around with the data while they get these courses and the boot camps. So um yeah we will just double check it uh with the with the with the committee and if there is no let's say objection then uh somewhere this week you're going to receive the data. The data is already available. Great. And uh another question about the submission of the work. How do we submit our work code and readmi file etc. Yes. So um for this one we would uh also when we release the data uh we would also give more specific instruction on discord uh on how to submit this you know through a maybe a zip file or something and then with some description. You also need to have a uh a uh video recording. So there are few things in the submission but for that I think the best thing is that we're going to give a um instruction on that on the discord that everybody will uh will see it and it's up to date. Yeah. Very good. And uh related to actually the sub challenges u is there a reason why we have challenge one and two not both combined in one challenge? Do we expect challenge two to use the output of challenge one? That's a very good question. So answering your final question, yes, it could be that actually the challenge to make a use of the challenge one information because if you have the possibility to already extract the right information and summarize the right information from your uh report, then you can already have the information that you need for the rag uh application. Uh the thing is of course in some of the well reports that we show it to you the idea is that this information required to do the notal analysis or coming in tables and the first part will be quite generic. We would ask also about the text that is already in the web uh in the web report. So the information about the drilling, the time of it, the name of the wells those are part of the summary right so that we would ask this and if already there you have the information to extract direct data from the table and we give it and give it to the let's say juries then already your second part is done. Yeah but we made it as as we said because this is already quite a lot of different steps and learnings for the participants. We say already if somebody can do the first rag based on this data and can give some generic information summarize some generic information from the wells there already they can get quite a lot of great next step will be about the TVDMD ID those parameters that we need for the models and again some cases it's also can be find in the text it's very easy to distinguish it some people need to have some modification to read the information from tables and assemble it and maybe some people would like to do it with the vision models that's also fine. We will look into it and we will grade it respectively. Yeah, thank you. Actually there is another question. It is also good to address it because it reminds about the motivation of this work. Uh will data security be also part of the challenge because although this data is open source in real usage data security would be a big issue. I I cannot agree more with this. This is a very very important aspect. Um at this stage I would say we would not um put it as a part of the challenge but uh look we for the previous years in the hackathon we had sometimes people who did something extra and we always take that into account. So if you make a submission that you say here I made some uh novelty by creating uh I don't know uh some workflows that is also secure or is doing some encryption in between. Make sure that you also put it in your submission because certainly you're going to get some extra grade if you do this. Yeah. So we don't put it an explicit part of the assignment but if you do it and if you dig into that part so you say I make a summary and I can do it in a secure way that is something that the juries will take into account. Yeah, but just to make the answer a little bit more explicit and to the point, it is not part of the challenge that we have uh defined. And maybe it is also good to mention that why we are insisting to use the open source models is that everything can also run locally. So this also help already a bit. Yeah, exactly. Thanks for the addition. Mhm. And uh another point about using multi- aent conversational framework that is not graph based like auto uh autogen or any of the challenges can they use them? Uh I wouldn't say why not. So yeah I mean why not? Of course the workflow you want to set up you're responsible for it. you it's your idea and well it would be great to also have some uh uh submissions with completely different approaches. Um we've put here some suggestion right so we say use a language model uh based on lama do but it's always like not limited to meaning that if you want to set up a multi- aent if you want to uh I don't know maybe take a model and fine-tune it whatever you want to do well this was a little bit of an extreme example but uh um that is something that you're open to do as long as the models are open source and do not require heavy GPU calculation in order to run the workflow. This is the most important thing that we ask about. Very good. Uh there was another question about the uh sharing this presentation. Yes, the answer is that the slides will be shared afterwards and also it is also available uh by recording the video. So you can watch it again and if there is something unclear maybe re-watching the video would also help and if not and still there is a question please uh let us know and we will be happy to help you through through it. Actually I do not see any more question in the chat box and yeah if there is no other burning questions now I think we are good to go now. Is there any more uh points or you wanted to emphasize on anything please? Um no I think as as said so on the discord channel if you still have question you get into the challenge you get the day time there are things that they don't work or it's not clear based on the explanation we gave uh please ask it on discord and if you see there are quite a lot of questions and similar questions we might also organize a Q&A session somewhere at the end of the boot camps so uh for us it's very important to get that feedback from you if something is not clear or if you still have doubts then let us know and then uh yeah we will react into it but yeah for now um we will share the presentation you will have the recording and uh I will say good luck with the with with your uh with your assignments yeah very good thank you very much for for the presentation and also addressing all the questions and yeah it was quite interesting and exciting to start the challenge and knowing about this and yeah also thank you for the participant and for the questions. It also helped to make it clear for the others. Definitely. Yeah. Thank you very much. Good luck and uh looking forward to see your submissions.