# Indexing vs Embeddings, LangChain, LangGraph - Complete Guide

**Date:** 2025-11-09

---

## Quick Reference Table

| Concept | What It Is | Role | We Use It? |
|---------|-----------|------|------------|
| **Embedding** | Text â†’ Numbers | Translation step | âœ… Yes (nomic-embed) |
| **Indexing** | Store + Search | Organization step | âœ… Yes (ChromaDB) |
| **Chunking** | Break into pieces | Preparation step | âœ… Yes (section-aware) |
| **Reranking** | Re-score results | Refinement step | âŒ No (use filters instead) |
| **LangChain** | LLM framework | Toolkit | âŒ No (built custom) |
| **LangGraph** | Agent framework | Brain | â³ Later (Sub-Challenge 3) |
| **Agentic AI** | Autonomous AI | Goal | â³ Later (Sub-Challenge 3) |

---

## Part 1: Indexing vs Embeddings

### The Relationship

**Think of it like a library:**
- **Embedding** = Translating book titles to catalog numbers
- **Indexing** = Creating the entire catalog system + organizing shelves

### Embeddings (Translation)

**What:** Convert text to numbers (vectors)

```
Input (Text):
"The well depth is 2,524 meters measured depth"

    â†“ Embedding Model (nomic-embed-text-v1.5)

Output (Vector - 768 numbers):
[0.234, -0.456, 0.789, 0.123, ..., -0.321]
```

**Why numbers?**
- Computers understand numbers
- Can calculate similarity (distance between vectors)
- Fast mathematical operations

**Example - Similarity:**
```
"well depth" â†’ [0.2, 0.5, 0.8, ...]
"depth measurement" â†’ [0.25, 0.48, 0.82, ...] â† Very similar!
"weather report" â†’ [-0.5, 0.1, -0.3, ...] â† Very different!
```

### Indexing (Organization + Storage)

**What:** The complete process of preparing documents for search

**Steps:**
1. **Chunk** documents into pieces
2. **Embed** each chunk (text â†’ numbers)
3. **Store** embeddings in database (ChromaDB)
4. **Add metadata** (section, page, type)
5. **Create search index** for fast retrieval

```
Full Indexing Pipeline:

PDF Document
    â†“
Chunking (1000 char pieces)
    â†“
[Chunk 1] [Chunk 2] [Chunk 3] ...
    â†“
Embedding (text â†’ vectors)
    â†“
[Vector 1] [Vector 2] [Vector 3] ...
    â†“
Store in ChromaDB
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ChromaDB Collection: "well_reports"      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Chunk  â”‚ Embeddingâ”‚ Text     â”‚ Metadata â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ W5_1   â”‚ [0.2...] â”‚ "Depth..." â”‚{page:6}â”‚
â”‚ W5_2   â”‚ [0.3...] â”‚ "Casing.."â”‚{page:12}â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**So:** Indexing includes embeddings + much more!

---

## Part 2: Our Chunking Strategy

### Section-Aware Chunking with Overlap

**Parameters:**
- Chunk size: **1000 characters**
- Overlap: **200 characters**
- Method: **Section-aware** (split by TOC sections)
- Context: **Prepend section header** to each chunk

### Visual Example

**Original Document:**
```
## 2.1 Depths

The measured depth (MD) is 2,524 meters. The true vertical
depth (TVD) is 2,523 meters. The well was drilled to target
the Delft formation at approximately 2,500m depth. The
formation consists of sandstone with good porosity...
(1500 characters total)
```

**After Chunking:**

**Chunk 1 (chars 0-1000):**
```
## 2.1 Depths

The measured depth (MD) is 2,524 meters. The true vertical
depth (TVD) is 2,523 meters. The well was drilled to target
the Delft formation at approximately 2,500m depth...
(1000 characters)

Metadata:
  section_number: "2.1"
  section_title: "Depths"
  section_type: "depth"
  page: 6
  chunk_index: 0
```

**Chunk 2 (chars 800-1800) - Note the 200 char overlap:**
```
## 2.1 Depths

...The well was drilled to target the Delft formation at
approximately 2,500m depth. The formation consists of
sandstone with good porosity and permeability...
(1000 characters starting at position 800)

Metadata:
  section_number: "2.1"
  section_title: "Depths"
  section_type: "depth"
  page: 6
  chunk_index: 1
```

### Why This Strategy?

**1. Section Headers Give Context**
```
âŒ Without header:
"The measured depth is 2,524 meters"
â†’ LLM doesn't know what document or section

âœ… With header:
"## 2.1 Depths
The measured depth is 2,524 meters"
â†’ LLM knows: depth data from section 2.1
```

**2. Overlap Prevents Information Loss**
```
Without overlap:
Chunk 1: "The well was drilled to target the Del-"
Chunk 2: "ft formation at 2,500m depth"
âŒ Sentence split awkwardly!

With 200-char overlap:
Chunk 1: "...the well was drilled to target the Delft..."
Chunk 2: "...target the Delft formation at 2,500m depth..."
âœ… Complete sentence in both chunks!
```

**3. Rich Metadata Enables Filtering**
```python
# Can search ONLY depth sections
results = vector_store.query(
    "What is the depth?",
    filters={'section_type': 'depth'}
)
# Returns only chunks with section_type = 'depth'
```

### Alternative Strategies We DON'T Use

**1. Fixed-Size Chunking (Simple)**
```python
# Split every 1000 chars, no context
chunks = [text[i:i+1000] for i in range(0, len(text), 1000)]
âŒ No section context
âŒ Splits sentences awkwardly
âŒ No metadata
```

**2. Sentence-Based Chunking**
```python
# Chunk by sentences
âŒ Chunks vary wildly in size
âŒ Some chunks too small (10 chars)
âŒ Some chunks too large (5000 chars)
```

**3. Paragraph-Based Chunking**
```python
# Chunk by paragraphs
âŒ Inconsistent chunk sizes
âŒ Still no section context
```

**Our approach (Section-Aware) is best for well reports!**

---

## Part 3: Reranking - We Don't Use It

### What is Reranking?

**Reranking = Second-pass scoring to reorder search results**

**Typical RAG with Reranking:**
```
Step 1: Query â†’ Embedding
Step 2: Vector Search â†’ Retrieve top 20 chunks (fast, less precise)
Step 3: Reranker Model â†’ Rescore all 20 chunks (slow, more precise)
Step 4: Return top 5 chunks (best ones)
Total Time: ~3-5 seconds
```

**Common Reranker Models:**
- Cohere Rerank
- Cross-Encoder models
- BAAI/bge-reranker

### Our Approach: Section Filtering (Better!)

```
Step 1: Query â†’ Intent Mapping â†’ Section Types
Step 2: Vector Search with Filters â†’ Retrieve top 5 (fast, precise)
Step 3: Return top 5 chunks
Total Time: ~1.4 seconds
```

**Example:**
```python
Query: "What is the well depth?"

# Without reranking (our approach):
1. Map query â†’ section_types = ['depth', 'borehole']
2. Search with filter:
   vector_store.query(
       query_embedding,
       filters={'section_type': {'$in': ['depth', 'borehole']}},
       n_results=5
   )
3. Return 5 chunks
Time: 1.4s

# With reranking (typical):
1. Search all chunks â†’ Get top 20
2. Rerank top 20 â†’ Get best 5
3. Return 5 chunks
Time: 3-5s
```

### Why Section Filtering > Reranking

| Aspect | Section Filtering | Reranking |
|--------|-------------------|-----------|
| **Speed** | Very fast (1.4s) | Slower (3-5s) |
| **Accuracy** | High (semantic filtering) | Slightly higher |
| **Complexity** | Low | High |
| **CPU Load** | Low | High |
| **Best For** | Small corpora (<1000 chunks) | Large corpora (>10,000 chunks) |

**For our use case:**
- âœ… 950 chunks (small) â†’ Don't need reranking
- âœ… Good embeddings (nomic-embed) â†’ Already accurate
- âœ… Speed requirement (<10s) â†’ Section filtering is faster
- âœ… TOC metadata â†’ Better than generic reranking

### When Would We Use Reranking?

**Only if:**
- Corpus grows to 10,000+ chunks
- Need cross-document reasoning
- Speed not critical
- Want to squeeze out 1-2% more accuracy

**Current verdict:** Section filtering is better! âœ…

---

## Part 4: LangChain - The Optional Toolkit

### What is LangChain?

**LangChain = Framework/library for building LLM applications**

Think of it as **LEGO blocks for AI apps**:
- Document loaders
- Text splitters
- Vector stores
- Chains (sequences of operations)
- Memory
- Agents

### LangChain Equivalent of Our System

**Our Custom Implementation:**
```python
# We built everything ourselves
from toc_parser import TOCEnhancedParser
from chunker import SectionAwareChunker
from embeddings import EmbeddingManager
from vector_store import TOCEnhancedVectorStore
from rag_system import WellReportRAG

parser = TOCEnhancedParser()
chunker = SectionAwareChunker(chunk_size=1000, overlap=200)
embedder = EmbeddingManager(model="nomic-ai/nomic-embed-text-v1.5")
vector_store = TOCEnhancedVectorStore()
rag = WellReportRAG()
```

**LangChain Version:**
```python
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA

loader = PyPDFLoader("well_report.pdf")
splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)
embeddings = HuggingFaceEmbeddings(model_name="nomic-ai/nomic-embed-text-v1.5")
vectorstore = Chroma.from_documents(docs, embeddings)
qa_chain = RetrievalQA.from_chain_type(llm, retriever=vectorstore.as_retriever())
```

### Why We Built Custom (Not LangChain)

**LangChain Pros:**
- âœ… Fast prototyping
- âœ… Many integrations
- âœ… Good documentation
- âœ… Community support

**LangChain Cons (for our use case):**
- âŒ Hard to do TOC-enhanced chunking
- âŒ Hard to add section metadata
- âŒ Abstractions hide details
- âŒ Extra dependencies
- âŒ Opinionated structure

**Our Custom Pros:**
- âœ… Full control over chunking
- âœ… TOC-aware processing
- âœ… Custom metadata (section types)
- âœ… Optimized for well reports
- âœ… No extra dependencies

**Trade-off:** More code, but better fit for our needs!

---

## Part 5: LangGraph - The Agent Brain

### What is LangGraph?

**LangGraph = Framework for building stateful agentic workflows**

Think of it as **state machine for AI agents**:
- Agents can be in different states
- Agents can transition between states
- Agents remember context
- Agents make decisions

### LangChain vs LangGraph

**LangChain (Linear Chain):**
```
A â†’ B â†’ C â†’ Done
Query â†’ Retrieve â†’ LLM â†’ Answer
```

**LangGraph (State Graph):**
```
         â”Œâ”€â”€â”€â”€â”€â”
    â”Œâ”€â”€â”€â”€â”‚Startâ”‚â”€â”€â”€â”€â”
    â”‚    â””â”€â”€â”€â”€â”€â”˜    â”‚
    â–¼               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”
â”‚Option â”‚       â”‚Option â”‚
â”‚  A    â”‚       â”‚  B    â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”˜       â””â”€â”€â”€â”¬â”€â”€â”€â”˜
    â”‚               â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
            â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Done? â”‚
        â””â”€â”€â”€â”¬â”€â”€â”€â”˜
            â”‚ No
            â–¼
        (Loop back)
```

### Our Use of LangGraph

**Sub-Challenge 1 (Current):** Simple RAG
- Linear flow: Query â†’ Retrieve â†’ Answer
- **Don't need LangGraph**
- Custom RAG is simpler

**Sub-Challenge 3 (Future):** Agentic Workflow
- Complex flow: Query â†’ Decide â†’ Use Tools â†’ Verify â†’ Loop
- **Will use LangGraph**
- Agent makes decisions

### Example: Sub-Challenge 3 Agent

```python
from langgraph.graph import StateGraph

# Define workflow
workflow = StateGraph()

# Add states (nodes)
workflow.add_node("understand_query", classify_intent)
workflow.add_node("query_rag", use_rag_tool)
workflow.add_node("extract_params", use_param_extractor)
workflow.add_node("run_nodal", use_nodal_analysis)
workflow.add_node("verify", check_completeness)

# Add transitions (edges)
workflow.add_conditional_edges(
    "understand_query",
    route_to_tool,
    {
        "need_rag": "query_rag",
        "need_params": "extract_params"
    }
)

workflow.add_conditional_edges(
    "verify",
    should_continue,
    {
        "continue": "query_rag",  # Loop back
        "done": END
    }
)

# Compile agent
agent = workflow.compile()

# Run
result = agent.run("Analyze well NLW-GT-03")
```

**Key Difference:** Agent decides what to do, can loop, verify, and adapt!

---

## Part 6: Agentic AI - The Goal

### What is Agentic AI?

**Agentic AI = AI that can perceive, decide, act, reflect, and adapt**

**Non-Agentic (Simple RAG - Sub-Challenge 1):**
```
User: "What is the well depth?"
System: [Always same process]
  1. Embed query
  2. Search vector DB
  3. LLM generates answer
  4. Return answer
Done.
```

**Agentic (Smart Agent - Sub-Challenge 3):**
```
User: "Analyze well performance for NLW-GT-03"
Agent: [Thinks and plans]

  "I need well information"
  â†’ Tool 1: Query RAG for well data
  â†’ Got: Well name, location, some depths

  "I need complete depth profile"
  â†’ Tool 1 again: Query RAG with section filter 'depth'
  â†’ Got: Full MD, TVD data

  "I need casing IDs"
  â†’ Tool 1 again: Query RAG with section filter 'casing'
  â†’ Got: Casing specifications

  "Now I can extract parameters"
  â†’ Tool 2: Parameter Extractor
  â†’ Got: {MD: [...], TVD: [...], ID: [...]}

  "Let me verify parameters are complete"
  â†’ Check: All fields present? âœ“

  "Now I can run nodal analysis"
  â†’ Tool 3: Nodal Analysis
  â†’ Got: Flow rate, BHP

  "Let me verify results make sense"
  â†’ Check: Flow rate > 0? âœ“
  â†’ Check: BHP reasonable? âœ“

  "All good! Generate comprehensive report"
  â†’ Done âœ“
```

**Key Difference:** Agent makes decisions, uses multiple tools, verifies, adapts!

---

## Part 7: How Everything Fits Together

### The Full Stack

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         AGENTIC AI SYSTEM                       â”‚
â”‚         (Sub-Challenge 3 - Future)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚     LangGraph        â”‚
        â”‚   (Agent Brain)      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚               â”‚               â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
â”‚ RAG    â”‚    â”‚ Param   â”‚    â”‚ Nodal    â”‚
â”‚ Tool   â”‚    â”‚ Extract â”‚    â”‚ Analysis â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚              â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚ Vector     â”‚ â”‚ LLM     â”‚
â”‚ Store      â”‚ â”‚ (Ollama)â”‚
â”‚ (ChromaDB) â”‚ â”‚         â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Embeddings â”‚
â”‚ (nomic)    â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Indexing   â”‚ â† We're working on this!
â”‚ (Chunking) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Layer by Layer

**Layer 1: Foundation (Indexing)**
- TOC extraction
- Section-aware chunking
- Embedding generation
- Vector storage

**Current Focus:** Re-indexing with TOC enhancement

**Layer 2: Retrieval (Vector Store)**
- ChromaDB with filters
- Section-type filtering
- Metadata search

**Status:** Working, 294 chunks â†’ 950 chunks (after re-index)

**Layer 3: Tools (RAG, Extractors)**
- RAG System (Sub-Challenge 1) âœ… Done
- Parameter Extractor (Sub-Challenge 2) â³ Next
- Nodal Analysis Wrapper (Sub-Challenge 2) â³ Next

**Layer 4: Agent (LangGraph)**
- Decision making
- Tool orchestration
- Verification loops

**Status:** Sub-Challenge 3 (future)

---

## Part 8: Why Indexing Matters for Agentic AI

### Bad Indexing = Dumb Agent

```
User: "Analyze well NLW-GT-03"

Agent: "I'll query RAG for well data"
  â†’ RAG searches
  â†’ 0 chunks found (bad indexing!) âŒ

Agent: "I have no data. Cannot proceed."
FAILED âŒ
```

### Good Indexing = Smart Agent

```
User: "Analyze well NLW-GT-03"

Agent: "I'll query RAG for well data"
  â†’ RAG searches
  â†’ 5 relevant chunks found (good indexing!) âœ“

Agent: "Great! Found depth data. Now extract parameters..."
  â†’ Parameter extraction succeeds âœ“

Agent: "Now run nodal analysis..."
  â†’ Analysis complete âœ“

SUCCESS âœ“
```

**Indexing is the foundation!** Without it, the agent is blind.

---

## Summary

### Quick Answers

**Q: What's the difference between embedding and indexing?**
A: Embedding = translation (text â†’ numbers). Indexing = full pipeline (chunk + embed + store + metadata).

**Q: What's our chunking strategy?**
A: Section-aware chunking with 1000 char chunks, 200 char overlap, section headers prepended, rich metadata.

**Q: Do we use reranking?**
A: No. Section filtering is better and faster for our use case.

**Q: Do we use LangChain?**
A: No. We built custom for TOC-enhancement and full control.

**Q: Do we use LangGraph?**
A: Not yet. Will use for Sub-Challenge 3 (agentic workflow).

**Q: How does indexing enable agentic AI?**
A: Good indexing = agents find data. Bad indexing = agents fail. It's the foundation!

---

## Current State & Next Steps

### What We Have
- âœ… Section-aware chunking (1000 chars, 200 overlap)
- âœ… TOC-enhanced parsing
- âœ… nomic-embed-text embeddings
- âœ… ChromaDB storage
- âœ… Custom RAG (no LangChain)
- âœ… 294 chunks indexed

### What We're Doing Now
- ğŸ”„ Re-indexing with TOC enhancement
- ğŸ”„ Increase to 950 chunks (14 PDFs)
- ğŸ”„ Add rich section metadata
- ğŸ”„ Fix 0-chunk queries

### What's Next
- â³ Build ground truth
- â³ Accuracy testing
- â³ Sub-Challenge 2 (parameter extraction)
- â³ Sub-Challenge 3 (LangGraph agent)

---

**The foundation (indexing) enables everything else!** ğŸ—ï¸
